% !TEX root = ./main.tex

\section{Linear Maps}

\subsection{Vector Space of Linear Maps}

Now, we may need more vector spaces, so let $V$ AND $W$ denoting vector spaces
over $\F$.

\begin{definition} [$\Polys{\F}{\empty}$]
    $\Polys{\F}{\empty}$ is the vector space of all polynomials with coefficients in $\F$.
\end{definition}

\begin{definition} [Linear Map]
   A \textbf{linear map} from $V$ to $W$ is a function
   $T : V \to W$ with the following properties:
   \begin{itemize}
       \item additivity: $T(u_1 + u_2) = Tu_1 + Tu_2$ for all $u_1, u_2 \in V$
       \item homogeneity: $T(\lambda u) = \lambda(Tu)$ for all $\lambda \in \F$ and all $u \in V$
   \end{itemize}
\end{definition}

For linear maps, we often use the
notation $Tu$ as well as the more standard functional notation
$T(u)$.

\begin{definition} [$\Lin{V, W}$]
   The set of all linear maps from $V$ to $W$ is denoted
   $\Lin{V, W}$. 
\end{definition}

\begin{example} Examples of Linear Maps:
   \begin{itemize}
       \item Zero: Define $0 \in \Lin{V,W}$ by $0u = 0$ for all $u \in V$.
       \item Identity map: Define $I \in \Lin{V,V}$ by $Iu = u$ for all $u \in V$.
       \item Differentiation: Define $D \in \Lin{\Polys{\R}{\empty}, \Polys{\R}{\empty}}$ by $Dp = p'$.
       \item Integration: Define $T \in \Lin{\Polys{\R}{\empty}, \R}$ by $Tp = \int_0^1 p(x) \dd{x}$.
       \item Multiplication by $x^2$: Define $T \in \Lin{\Polys{\R}{\empty}, \Polys{\R}{\empty}}$ by
       \[ (Tp)(x) = x^2 p(x) \]
       for $x \in \R$.
       \item Backward shift: Define $T \in \Lin{\F^{\infty}, \F^{\infty}}$ by
       \[ T(x_1, x_2, x_3, \dots) = (x_2, x_3, \dots). \]
       \item From $\R^3$ to $\R^2$: Define $T \in \Lin{\R^3, \R^2}$ by
       \[ T(x, y, z) = (2x - y + 3z, 7x + 5y - 6z). \]
   \end{itemize} 
\end{example}

\begin{theorem}
    Suppose $\listofvectors$ is a basis of $V$ and $\listofnames{w}{n} \in W$. Then there
    exists a unique linear map $T : V \to W$ such that
    \[ Tv_j = w_j \]
    for each $j = 1, \dots, n$.

    \begin{proof*}
        Define $T: V \to W$ by
        \[ T(\linearcombination) = a_1w_1 + \dots + a_nw_n, \]
        where $\listofscalars$ are arbitrary elements of $\F$.

        It is straightforward to check the above map is additive, just take all the
        coefficients except $a_i$ to be 0. The distributive property handles homogeneity.

        There cannot be another such map because if you add all the constraints
        together, you get precisely this relation. \qed
    \end{proof*}
\end{theorem}

\begin{definition} [Addition and Scalar Multiplication on $\Lin{V, W}$]
   Suppose $S, T \in \Lin{V, W}$ and $\lambda \in \F$. The sum $S+T$ is defined as:
   \[ (S+T)(u) = Su + Tu \]
    and the product $\lambda T$ is defined as:
    \[ (\lambda T)(u) = \lambda (Tu) \]
    for all $u \in V$.

    Clearly, these maps are also linear maps, thus stay in the set.
\end{definition}

\begin{note} [$\Lin{V, W}$ is a Vector Space]
   With the operations of addition and scalar multiplication as defined above, $\Lin{V, W}$
   is a vector space. 
\end{note}

\begin{definition} [Product of Linear Maps]
   If $T \in \Lin{U, V}$ and $S \in \Lin{V, W}$, then the product $ST \in \Lin{U, W}$ is defined by
   \[ (ST)(u) = S(Tu) \]
   for $u \in U$.
\end{definition}

\begin{note} [Algebraic Properties of Products of Linear Maps]
   \begin{itemize}
       \item Associativity: $(T_1 T_2)T_3 = T_1(T_2 T_3)$
       \item Identity: $TI = IT = T$ (note this may be two different $I$'s)
       \item Distributive Properties: $(S_1 + S_2)T = S_1 T + S_2 T$ and $S(T_1 + T_2) = ST_1 + ST_2$
   \end{itemize} 
\end{note}

\begin{theorem} [Linear Maps take 0 to 0]
   Suppose $T$ is a linear map from $V$ to $W$. Then $T(0) = 0$. 
\end{theorem}

There's a tricky bit about the word "linear". In calculus, we say any
$f(x) = mx + b$, this is termed linear. However, in the sense of vector spaces,
this function is only linear if and only if $b = 0$.

\subsection{Null Spaces and Ranges}

\begin{definition} [Null Space]
   For $T \in \Lin{V, W}$, the \textbf{null space} of $T$, denoted $\null T$, is the subset of $V$
   containing those vectors that $T$ maps to 0:
   \[ \Null T = \{ u \in V : Tu = 0 \} \]
\end{definition}

\begin{example} Examples of Null Spaces:
   \begin{itemize}
      \item Suppose $T$ is the zero map form $V$ to $W$; in other words, $Tu = 0$ for
      every $u \in V$. Then $\Null T = V$.
      \item Suppose $\phi \in \Lin{\C^3, \C}$ is defined by $\phi(z_1, z_2, z_3) = z_1 + 2z_2 + 3z_3$.
      Then $\Null \phi = \{ (z_1, z_2, z_3) \in \C^3 : z_1 + 2z_2 + 3z_3 = 0 \}$.
      \item Consider $D$, the differentiation map. The only functions whose derivative equals zero
      is the constant functions. Thus, the null space is the set of all constant functions.
   \end{itemize}
\end{example}

\begin{theorem} [The Null Space is a Subspace]
   Suppose $T \in \Lin{V, W}$. Then $\Null T$ is a subspace of $V$.
\end{theorem}

\begin{definition} [Injective]
   A function $T : V \to W$ is called \textbf{injective} or \textbf{one-to-one} if
   $Tu = Tv$ implies $u = v$.
\end{definition}

\begin{theorem}
   Let $T \in \Lin{V, W}$. Then $T$ is injective if and only if $\Null T = 0$.
\end{theorem}

\begin{definition} [Range]
   For $T \in \Lin{V, W}$, the \textbf{range} of $T$ is the subset
   of $W$ consisting of those vectors that are of the form $Tu$ for some $u \in V$:
   \[ \range T = \{ Tu : u \in V \} \]
\end{definition}

\begin{example} Ranges:
   \begin{itemize}
      \item Suppose $T$ is the zero map from $V$ to $W$; in other words,
      $Tu = 0$ for every $u \in V$. Then $\range T = \{ 0 \}$.
      \item Suppose $T \in \Lin{\R^2, \R^3}$ is defined by $T(x,y) = (2x, 5y, x+y)$,
      then $\range T = \{ (2x, 5y, x+y) : x, y \in \R \}$. A basis of $\range{T}$ is $(2, 0, 1)$, $(0, 5, 1)$.
      \item Consider the differentiation map $D \in \Lin{\Polys{\R}{\empty},\Polys{\R}{\empty}}$. Since every polynomial $q \in \Polys{\R}{\empty}$
      has a polynomial $p \in \Polys{\R}{\empty}$ such that $p' = q$, the range of $D$ is $\Polys{\R}{\empty}$.
   \end{itemize}
\end{example}

\begin{theorem} [The Range is a Subspace]
   If $T \in \Lin{V, W}$, then $\range T$ is a subspace of $W$.
\end{theorem}

\begin{definition} [Surjective]
   A function $T : V \to W$ is called \textbf{surjective} or \textbf{onto}
   if its range equals $W$.
\end{definition}

\begin{theorem} [Fundamental Theorem of Linear Maps]
   Suppose $V$ is finite-dimensional and $T \in \Lin{V, W}$. Then
   \[ \dim V = \dim \Null T + \dim \range T. \]
   \begin{proof*}
      (Proof Sketch)

      Let $u_1, \dots, u_m$ be a basis of $\Null T$; thus $\dim \Null T = m$.

      The linear independent list $u_1, \dots u_m$ can be extended to a basis
      \[ \listofnames{u}{m}, \listofvectors \]

      Thus $\dim V = m + n$.
      To complete the proof, we need to show that $\dim \range T = n$. We do this by proving that $Tv_1, \dots, TV_n$
      is a basis of $\range T$. \qed
   \end{proof*}
\end{theorem}

\begin{theorem} [A Map to a Smaller Dimensional Space is Not Injective]
   Suppose $V$ and $W$ are finite-dimensional vector spaces such that
   $\dim V > \dim W$. Then no linear map from $V$ to $W$ is injective.
   \begin{proof*}
      Suppose $T \in \Lin{V, W}$. Because
      \[ \dim V = \dim \Null T + \dim \range T \]
      and
      \[ \dim V > \dim W \geq \dim \range T \]
      we have $\dim \Null T > 0$. Thus $T$ is not injective. \qed
   \end{proof*}
\end{theorem}

\begin{theorem} [A Map to a Larger Dimensional Space is Not Surjective]
   Suppose $V$ and $W$ are finite-dimensional vector spaces such that
   $\dim V < \dim W$. Then no linear map from $V$ to $W$ is surjective.

   \begin{proof*}
      Suppose $T \in \Lin{V, W}$. Because
      \[ \dim V = \dim \null T + \dim \range T \]
      we have
      \[ \dim \range T \leq \dim V < \dim W \]

      Thus, $T$ is not surjective. \qed
   \end{proof*}
\end{theorem}

Now we can use these results to prove some facts about a related
subject, the theory of systems of linear equations.

\begin{definition} [Homogenous Linear Equations]
   Fix positive integers $m$ and $n$ and let $A_{j, k} \in \F$ for
   $j = 1, \dots, m$ and $k = 1, \dots, n$. Consider
   the homogeneous system of linear equations
   \begin{align*}
      \sum_{k=1}^n A_{1,k}x_k &= 0 \\
      &\vdots
      \sum_{k=1}^n A_{m,k}x_k &= 0
   \end{align*}

   These are called homogenous because the constant terms are
   all 0.
\end{definition}

We wish to ask the following: do there exist solutions other than the
trivial solution, i.e. $x_1 = \dots = x_n = 0$?

Define $T: \F^n \to \F^m$ by
\[ T(x_1, \dots, x_n) = \qty(\sum_{k=1}^n A_{1,k}x_k, \dots, \sum_{k=1}^n A_{m,k}x_k) \]

The equation $T(x_1, \dots, x_n) = 0$ is the same as the homogeneous
system of linear equations above. This is asking if $\null T = 0$,
which is the same asking: is $T$ injecive?

Well, we know $T$ is not injective if $\dim \F^n > \dim \F^m$, in other words
if $n > m$, so we have the following result:

\begin{theorem} [Homogenous System of Linear Equations]
   A homogeneous system of linear equations with more
   variables than equations has nonzero solutions.
\end{theorem}

Now, let us talk about other types of systems of linear equations.

\begin{definition} [Inhomogenous Linear Equations]
   Fix positive integers $m$ and $n$ and let $A_{j, k} \in \F$ for
   $j = 1, \dots, m$ and $k = 1, \dots, n$. Consider
   the inhomogeneous system of linear equations
   \begin{align*}
      \sum_{k=1}^n A_{1,k}x_k &= c_1 \\
      &\vdots
      \sum_{k=1}^n A_{m,k}x_k &= c_m
   \end{align*}

   These are called inhomogenous because the constant terms are
   not all 0.
\end{definition}

Now we wonder the following:
is there some choice of $\listofnames{c}{m} \in \F$ such that
no solution exists?

Define $T: \F^n \to \F^m$ by

\[ T(x_1, \dots, x_n) = \qty(\sum_{k=1}^n A_{1,k}x_k, \dots, \sum_{k=1}^n A_{m, k}x_k) \]

The equation $T(x_1, \dots, x_n) = (\listofnames{c}{m})$ is the same
as the inhomogeneous system of linear equations above. This is the same
as asking: is $T$ surjective?

We know $T$ is not surjective if $m > n$ (similar to previous logic),
so we have the following result:

\begin{theorem} [Inhomogenous System of Linear Equations]
   An inhomogeneous system of linear equations with more equations
   than variables has no solution for some choice of
   constant terms.
\end{theorem}

\subsection{Matrices}

\begin{definition} [Matrix]
   Let $m$ and $n$ denote positive integers. An $m$-by-$n$ \textbf{matrix}
   $A$ is a rectangular array of elements of $\F$ with $m$ rows and $n$ columns:

   $A = \begin{pmatrix}
     A_{1,1} && \dots && A_{1,n} \\
     \vdots && \empty && \vdots \\
     A_{m,1} && \dots && A_{m,n} 
   \end{pmatrix}$

   The notation $A_{j,k}$ denotes the entry in row $j$, column $k$ of $A$.
\end{definition}

The first index refers to the row numbers and the second index
refers to column numbers.

Thus $A_{2,3}$ refers to the entry in the second row, third column of $A$.

\begin{definition} [Matrix of a Linear Map, $\Matof{T}$]
   Suppose $T \in \Lin{V,W}$ and $\listofvectors$ is a basis of $V$ and
   $\listofnames{w}{m}$ is a basis of $W$. The \textbf{matrix} of $T$ with respect
   to these bases is the $m$-by-$n$ matrix $\Matof{T}$ whose entries
   $A_{j, k}$ are defined by
   \[ Tv_k = A_{1, k}w_1 + \dots + A_{m,k}w_m \]

   If the bases are not clear from the context, then the notation
   \[ \mathcal{M}(T, (\listofvectors), (\listofnames{w}{m})) \]
   is used.
\end{definition}

\includegraphics{matrix intuition.png}

To understand what the matrix really means, fix a column $k$
the $k$th column of $\Matof{T}$ consists of the scalars needed to write
\[ Tv_k = \sum_{j=1}^m A_{j,k}w_j.\]

The picture above should remind you that $Tv_k$ can be computed from
$\Matof{T}$ by multiplying each entry in the $k$th column by the corresponding
$w_j$ from the left column, and then adding up the resulting vectors.

\begin{example} [Matrices]
   Suppose $T \in \Lin{\F^2, \F^3}$ is defined by:
   \[ T(x,y) = (x+3y, 2x+5y, 7x+9y) \]
   Because $T(1, 0) = (1, 2, 7)$ and $T(0, 1) = (3, 5, 9)$, the matrix of
   $T$ with respect to the standard bases is the 3-by-2 matrix
   \[ \Matof{T} = \begin{pmatrix}
      1 & 3 \\
      2 & 5 \\
      7 & 9
   \end{pmatrix} \]

   Suppose $D \in \Lin{\Polys{\R}{3}, \Polys{\R}{2}}$ is the differentiation
   map. Beacuse $(x^n)' = nx^{n-1}$, the matrix of $D$ with respect
   to the standard bases of $\Polys{\R}{3}$ and $\Polys{\R}{2}$ is the
   3-by-4 matrix
   \[ \Matof{D} = \begin{pmatrix}
      0 & 1 & 0 & 0 \\
      0 & 0 & 2 & 0 \\
      0 & 0 & 0 & 3 \\
   \end{pmatrix}\]
\end{example}

Note that $\Matof{T}$ contains all the information about $T$.
The matrix contains all the information about how the
basis vectors transform under $T$, however, under linearity,
we can write every vector in $V$ as a linear combination of the basis
and since $T$ is linear we can see how any vector transforms.

\begin{definition} [Matrix Addition]
   The sum of two matrices of the same size is the matrix obtained
   by adding corresponding entries in the matrices:

   \[ \begin{pmatrix}
      A_{1,1} && \ldots && A_{1,n} \\
      \vdots && \empty && \vdots \\
      A_{m,1} && \dots && A_{m,n}
   \end{pmatrix}
   +
   \begin{pmatrix}
      C_{1,1} && \ldots && C_{1,n} \\
      \vdots && \empty && \vdots \\
      C_{m,1} && \dots && C_{m,n}
   \end{pmatrix} \]

   \[ = \begin{pmatrix}
      A_{1,1} + C_{1,1} && \ldots && A_{1,n} + C_{1,n}  \\
      \vdots && \empty && \vdots \\
      A_{m,1} + C_{m,1}  && \dots && A_{m,n} + C_{m,n}
   \end{pmatrix} \]
   
   In other words, $(A + C)_{j,k} = A_{j,k} + C_{j,k}$.
\end{definition}

In the following result, we assume that the same bases are used
for $\Matof{S+T}$, $\Matof{S}$, and $\Matof{T}$.

\begin{theorem} [Addition of Matrices]
   Suppose $S,T \in \Lin{V, W}$. Then $\Matof{S+T} = \Matof{S} + \Matof{T}$.
\end{theorem}

\begin{definition} [Scalar Multiplication of a Matrix]
   The produce of a scalar and a matrix is the matrix
   obtained by multiplying each entry in the matrix by the scalar:
   \[ \lambda \begin{pmatrix}
      A_{1,1} && \ldots && A_{1,n} \\
      \vdots && \empty && \vdots \\
      A_{m,1} && \dots && A_{m,n}
   \end{pmatrix}
   =
   \begin{pmatrix}
      \lambda A_{1,1} && \ldots && \lambda A_{1,n} \\
      \vdots && \empty && \vdots \\
      \lambda A_{m,1} && \dots && \lambda A_{m,n}
   \end{pmatrix}
   \]

   In other words, $(\lambda A)_{j,k} = \lambda A_{j,k}$.
\end{definition}

In the following result, we assume that the same bases are used
for $\Matof{\lambda T}$ and $\Matof{T}$.

\begin{theorem} [The Matrix of a Scalar Times a Linear Map]
   Suppose $\lambda \in \F$ and $T \in \Lin{V, W}$. Then
   $\Matof{\lambda T} = \lambda \Matof{T}$.
\end{theorem}

\begin{note} [$\F^{m,n}$]
   For $m$ and $n$ positive integers, the set of all $m$-by-$n$ matrices
   with entries in $\F$ is denoted by $\F^{m,n}$.
\end{note}

\begin{theorem} [$\dim \F^{m,n} = mn$]
   Suppose $m$ and $n$ are positive integers. With addition and scalar
   multiplication defined as above, $\F^{m, n}$ is a vector space with dimension
   $mn$.
\end{theorem}

Now we want to define matrix multiplication.

Consider the following vector spaces:
\begin{itemize}
   \item $V$ with basis $\listofvectors$.
   \item $W$ with basis $\listofnames{w}{m}$.
   \item $U$ with basis $\listofnames{u}{p}$.
\end{itemize}

Consider linear maps $T: U \to V$ and $S: V \to W$. We
want to define matrix multiplication such that
\[ \Matof{ST} = \Matof{S}\Matof{T} \]

We can try $(AB)_{j,k} = A_{j,k} \times B_{j,k}$, however
this will not line up with our definition for linear maps.

Instead consider the following, suppose $\Matof{S} = A$ and
$\Matof{T} = C$.

For $1 \leq k \leq p$, we have

\begin{align*}
   (ST)u_k &= S \qty(\sum_{r=1}^n C_{r,k}v_r) \\
   &= \sum_{r=1}^n C_{r,k}Sv_r \\
   &= \sum_{r=1}^n C_{r,k} \sum_{j=1}^m A_{j,r}w_j \\
   &= \sum_{j=1}^m \qty(\sum_{r=1}^n A_{j,r} C_{r,k}) w_j. 
\end{align*}

Thus, $\Matof{ST}$ is the $m$-by-$p$ matrix whose entry in row
$j$, column $k$, is

\[ (AC)_{j,k} = \sum_{r=1}^n A_{j,r} C_{r,k} \]

We can collect this in the following result:

\begin{definition}
   Suppose $A$ is an $m$-by-$n$ matrix and $C$ is an $n$-by-$p$ matrix.
   Then, $AC$ is the $m$-by-$p$ matrix whose entry in row $j$, column
   $k$, is

   \[ (AC)_{j,k} = \sum_{r=1}^n A_{j,r} C_{r,k} \]
\end{definition}

The entry in row $j$, column $k$, of $AC$ is computed
by taking row $j$ of $A$ and column $k$ of $C$, multiplying
together corresponding entries, and then summing.

In addition, matrix multiplication is only defined if
the amount of columns of $A$ is the same as the amount
of columns of $C$.

In the following result, the same bases are used in considering
linear maps with shared vector spaces.

\begin{definition}
   Suppose $A$ is an $m$-by-$n$ matrix.
   \begin{itemize}
      \item If $1 \leq j \leq m$, then $A_{j, \cdot}$ denotes
      the $1$-by-$n$ matrix consisting of row $j$ of $A$.
      \item If $1 \leq k \leq n$, then $A_{\cdot, k}$ denotes
      the $m$-by-$1$ matrix consisting of column $k$ of $A$.
   \end{itemize}
\end{definition}

Here are some alternate ways to think about matrix multiplication.

\begin{theorem} 
   Suppose $A$ is an $m$-by-$n$ matrix and $C$ is an $n$-by-$p$ matrix. Then

   \[ (AC)_{j,k} = A_{j, \cdot} C_{\cdot, k} \]
   \[ (AC)_{\cdot, k} = AC_{\cdot, k} \]
   \[ (AC)_{j, \cdot} = A_{j, \cdot} C \]
\end{theorem}

\begin{theorem} [Linear Combination of Columns]
   Suppose $A$ is an $m$-by-$n$ matrix and $c = \begin{pmatrix*} c_1 \\ \vdots \\ c_n \end{pmatrix*}$
   is an $n$-by-1 matrix. Then \[Ac = c_1 A_{\cdot, 1} + \dots + c_n A_{\cdot, n}\]
\end{theorem}

In other words, $Ac$ is a linear combination of the columns of $A$,
with the scalars that multiply the columns coming from $c$.

\begin{theorem}
   Suppose $a = \begin{pmatrix*} a_1 & \dots & a_n \end{pmatrix*}$ is
   a 1-by-$n$ matrix and $C$ is an $n$-by-$p$ matrix. Then
   \[ aC = a_1 C_{1, \cdot} + \dots + a_n C_{n, \cdot} \]
\end{theorem}

In other words, $aC$ is a linear combination of the rows of $C$,
with the scalars that multiply the rows coming from $a$.

\subsection{Invertibility and Isomorphic Vector Spaces}

\begin{definition} [Invertible, Inverse]
   A linear map $T \in \Lin{V, W}$ is called \textbf{invertible} if
   there exists a linear map $S \in \Lin{W, V}$ such that $ST$ equals
   the identity map on $V$ and $TS$ equals the identity map on $W$.

   Such an $S$ is called the \textbf{inverse} of $T$. We can denote
   it using $S = T^{-1}$.
\end{definition}

\begin{theorem} [The Inverse is Unique]
   An invertible linear map has a unique inverse.
\end{theorem}

\begin{theorem}
   A linear map is invertible if and only if it is injective and surjective.
\end{theorem}

\begin{definition} [Isomorphism, Isomorphic]
   An invertible linear map is called a \textbf{isomorphism}. Two
   vector spaces are called \textbf{isomorphic} if there is
   an isomorphism from one vector space onto the other.
\end{definition}

Think of an isomorphism as a way to relabel vectors in a space.

\begin{theorem} [Dimension shows Isomorphic]
   Two finite-dimensional vector spaces over $\F$ are isomorphic
   if and only if they have the same dimension.
\end{theorem}

\begin{theorem}[$\Lin{V, W}$ and $\F^{m, n}$ are Isomorphic]
   Suppose $\listofvectors$ is a basis of $V$ and $\listofnames{w}{m}$
   is a basis of $W$. Then $\mathcal{M}$ is an isomorphism
   between $\Lin{V, W}$ and $\F^{m, n}$.

\end{theorem}

\begin{theorem}
   Suppose $V$ and $W$ are finite-dimensional. Then $\Lin{V, W}$ is
   finite-dimensional and 
   \[ \dim \Lin{V, W} = (\dim V)(\dim W) \]
\end{theorem}

\begin{definition} [Matrix of a Vector]
   Suppose $u \in V$ and a basis of $V$ as $\listofvectors$. The
   \textbf{matrix} of $u$ with respect to this basis is the $n$-by-1 matrix
   \[ \Matof{u} = 
   \begin{pmatrix}
      c_1 \\ \vdots \\ c_n
   \end{pmatrix} \]
   where $c_1, \dots, c_n$ are the scalars such that
   \[ u = c_1v_1 + \dots + c_nv_n \]
\end{definition}

\begin{example}
   The matrix of $3 - 7x + 5x^2$ with respect to the 
   standard basis of $\Polys{\R}{2}$ is
   \[\begin{pmatrix}
      3 \\ -7 \\ 5
   \end{pmatrix} \]
\end{example}

\begin{theorem} [Linear Maps act as Matrix Multiplication]
   Suppose $T \in \Lin{V, W}$ and $u \in V$. Suppose $\listofvectors$
   is a basis of $V$ and $\listofnames{w}{m}$ is a basis of $W$. Then
   \[ \Matof{Tu} = \Matof{T}\Matof{u} \]
\end{theorem}

\begin{definition} [Operator, $\Lin{V}$]
   We use operators to describe a specific type of map. 
   \begin{itemize}
      \item A linear map from a vector space to itself is called an \textbf{operator}.
      \item The notation $\Lin{V}$ denotes the set of all operators on $V$. In other words,
      $\Lin{V} = \Lin{V, V}$
   \end{itemize}
\end{definition}

\begin{theorem}
   Suppose $V$ is finite-dimensional and $T \in \Lin{V}$. Then the following
   are equivalent:
   \begin{itemize}
      \item $T$ is invertible;
      \item $T$ is injective;
      \item $T$ is surjective;
   \end{itemize}
\end{theorem}

\subsection{Products and Quotients of Vector Spaces}

\begin{definition} [Product of Vector Spaces]
   Suppose $V_1, \dots, V_m$ are vector spaces over $\F$.
   \begin{itemize}
      \item The \textbf{product} $V_1 \cross \dots \cross V_m$ is defined
      by \[ V_1 \cross \dots \cross V_m = \{(u_1, \dots, u_m) : u_1 \in V_1, \dots, u_m \in V_m \} \]
      \item Addition on $V_1 \cross \dots \cross V_m$ is defined by
      \[ (u_1, \dots, u_m) + (w_1, \dots, w_m) = (u_1 + w_1, \dots, u_m + w_m) \]
      \item Scalar multiplication on $V_1 \cross \dots \cross V_m$ is defined by
      \[ \lambda(u_1, \dots, u_m) = (\lambda u_1, \dots, \lambda u_m) \]
   \end{itemize}
\end{definition}

\begin{theorem} [Product of Vector Spaces is a Vector Space]
   Suppose $V_1, \dots V_m$ are vector spaces over $\F$. Then $V_1 \cross \dots \cross V_m$
   is a vector space over $\F$.
\end{theorem}

\begin{example} Example of Product of Vector Spaces
   \begin{align*}
      \R^2 \cross &\R^3 = \{ ((x_1, x_2), (x_3, x_4, x_5)) : x_1, x_2, x_3, x_4, x_5 \in \R \} \\
      &\R^5 = \{ (x_1, x_2, x_3, x_4, x_5) : x_1, x_2, x_3, x_4, x_5 \in \R \}
   \end{align*}
   Note that $\R^2 \cross \R^3 \neq \R^5$.
   However, they are very similar.

   The linear map $T: \R^2 \cross \R^3 \to \R^5$ defined by
   \[ T((x_1, x_2), (x_3, x_4, x_5)) = (x_1, x_2, x_3, x_4, x_5) \]
   is clearly an isomorphism of $\R^2 \cross \R^3$ onto $\R^5$. Thus
   these two vector spaces are clearly isomorphic.
\end{example}

\begin{theorem} [Dimensional of a Product is the Sum of Dimensions]
   Suppose $V_1, \dots, V_m$ are finite-dimensional vector spaces.
   Then $V_1 \cross \dots \cross V_m$ is finite-dimensional, and
   \[ \dim(V_1, \cross \dots \cross V_m) = \dim V_1 + \dots + \dim V_m \]
\end{theorem}

\begin{theorem}
   Suppose $U_1, \dots, U_m$ are subspaces of $V$.
   Define a linear map $\Gamma: U_1 \cross \dots \cross U_m \to U_1 + \dots + U_m$ by
   \[ \Gamma(u_1, \dots, u_m) = u_1 + \dots + u_m. \]
   Then $U_1 + \dots + U_m$ is a direct sum if and only if $\Gamma$ is injective.
\end{theorem}

\begin{theorem}
   Suppose $V$ is finite-dimensional and $U_1, \dots, U_m$ are subspaces of $V$.
   Then $U_1 + \dots + U_m$ is a direct sum if and only if
   \[ \dim(U_1 + \dots + U_m) = \dim U_1 + \dots + \dim U_m \]
\end{theorem}

\begin{definition}
   Suppose $v \in V$ and $U$ is a subspace of $V$. Then $v + U$ is
   the subset of $V$ defined by
   \[ v+U = \{v+u : u \in U \} \]
\end{definition}

\begin{definition} [Affine Subset, Parallel]
   An \textbf{affine subset} of $V$ is a subset of $V$ of the form
   $v + U$ for some $v \in V$ and some subspace $U$ of $V$.

   This affine subset is said to be \textbf{parallel} to $U$.
\end{definition}

\begin{definition}
   Suppose $U$ is a subspace of $V$. Then the \textbf{quotient space}
   $V / U$ is the set of all affine subsets of $V$ parallel to $U$. In other words,
   \[ V/U = \{ v+U : v \in V \} \]
\end{definition}

\begin{example} Examples of Quotient Spaces
   \begin{itemize}
      \item If $U = \{(x, 2x) \in \R^2 : x \in \R \}$, then $\R^2 / U$ is the set of all lines
      in $\R^2$ that have slope 2.
      \item If $U$ is a plane in $\R^3$ containing the origin, then $R^3/U$ is the set of all planes
      in $\R^3$ parallel to $U$
   \end{itemize}
\end{example}

\begin{theorem}
   Suppose $U$ is a subspace of $V$ and $v, w \in V$. The following are equivalent:
   \begin{enumerate}
      \item $v - w \in U$
      \item $v + U = w + U$
      \item $(v + U) \cap (w + U) \neq \emptyset$
   \end{enumerate}
\end{theorem}

\begin{definition} [Addition and Scalar Multiplication on $V/U$]
   Suppose $U$ is a subspace of $V$. Then \textbf{addition} and
   \textbf{scalar multiplication} are defined on $V / U$ by
   \[ (v + U) + (w + U) = (v + w) + U \]
   \[ \lambda(v + U) = (\lambda v) + U \]
   for $v, w \in V$ and $\lambda \in \F$.
\end{definition}

\begin{theorem} [Quotient Space is a Vector Space]
   Suppose $U$ is a subspace in $V$. Then $V/U$, with the operations
   of addition and scalar multiplication as defined above,
   is a vector space.
\end{theorem}

\begin{definition} [Quotient Map]
   Suppose $U$ is a subspace of $V$. The \textbf{quotient map} $\pi$
   is the linear map $\pi : V \to V/U$ defined by
   \[ \pi(v) = v + U \]
   for $v \in V$.
\end{definition}

\begin{theorem} [Dimension of a Quotient Space]
   Suppose $V$ is a finite-dimensional and $U$ is a subspace of $V$. Then
   \[ \dim V/U = \dim V - \dim U \]

   \begin{proof*}
      \begin{align*}
         \dim V &= \dim \Null \pi + \dim \range \pi \\
         &= \dim U + \dim V/U
      \end{align*}
      which immediately shows the result. \qed
   \end{proof*}
\end{theorem}

\begin{definition} [The Induced Map over a Quotient Space]
   Suppose $T \in \Lin{V, W}$. Define the \textbf{induced map} $\tilde{T} : V / (\Null T) \to W$ by
   \[ \tilde{T}(v + \Null T) = Tv \]
\end{definition}

To show that the definition of $\tilde{T}$ makes sense, suppose $u,v \in V$
are such that $u + \Null T = v + \Null T$. By a previous theorem, we have
that $u - v \in \Null T$ meaning $T (u-v) = 0$ and so $Tu = Tv$, which makes sense.

Providing this in the other way shows injectivity. In fact, we have
the following properties of $\tilde{T}$:

\begin{theorem}
   Suppose $T \in \Lin{V, W}$. Then
   \begin{enumerate}
      \item $\tilde{T}$ is a linear map from $V / (\Null T)$ to $W$.
      \item $\tilde{T}$ is injective.
      \item $\range \tilde{T} = \range T$.
      \item $V / (\Null T)$ is isomorphic to $\range T$.
   \end{enumerate}
\end{theorem}

\subsection{Duality}

\begin{definition} [Linear Functional]
   A \textbf{Linear Functional} on $V$ is a linear map
   from $V$ to $\F$. In other words, a linear functional is
   an element of $\Lin{V, \F}$.
\end{definition}

\begin{example} Examples of linear functionals:
   \begin{itemize}
      \item Define $\phi: \R^3 \to \R$ by $\phi(x, y, z) = 4x-5y+2z$. Then
      $\phi$ is a linear functional on $\R^3$.
      \item Define $\phi: \PolysAll{\R} \to \R$ by $\phi(p) = \int_{0}^1 p(x) \dd{x}$. Then
      $\phi$ is a linear functional on $\PolysAll{\R}$.
   \end{itemize}
\end{example}

\begin{definition} [Dual Space]
   The \textbf{dual space} of $V$, denoted $V'$, is the vector space
   of all linear functionals on $V$. In other words, $V' = \Lin{V, \F}$.
\end{definition}

\begin{theorem} [$\dim V' = \dim V$]
   Suppose $V$ is finite-dimensional. Then $V'$ is also finite-dimensional
   and $\dim V' = \dim V$.

   \begin{proof*}
      \begin{align*}
         \dim V' &= \dim \Lin{V, \F} \\
         &= (\dim V)(\dim \F) \\
         &= \dim V
      \end{align*}
   \end{proof*}
\end{theorem}

\begin{definition}
   If $v_1, \dots, v_n$ is a basis of $V$, then the \textbf{dual basis}
   of $v_1, \dots, v_n$ is the list $\phi_1, \dots, \phi_n$ of elements
   of $V'$, where each $\phi_j$ is the linear functional on $V$ such that

   \[ \phi_j(v_k) = 
   \begin{cases}
      1 & \text{if } k = j \\
      0 & \text{if } k \neq j 
   \end{cases}
   \]
\end{definition}

\begin{example}
   Consider the standard basis $e_1, \dots, e_n$ of $\F^n$.
   For $1 \leq j \leq n$, define $\phi_j$ by
   \[ \phi_j(x_1, \dots, x_n) = x_j \]
   for $(x_1, \dots, x_n) \in \F^n$. Clearly
   \[ \phi_j(e_k) = \begin{cases}
      1 & \text{if } k = j \\
      0 & \text{if } k \neq j 
   \end{cases} \]
   Thus, $\phi_1, \dots, \phi_n$ is the dual basis of the
   standard basis $e_1, \dots, e_n$ of $\F^n$.
\end{example}

\begin{theorem}
   Suppose $\dim V < \infty$. Then the dual basis
   of a basis of $V$ is a basis of $V'$.
\end{theorem}

\begin{definition} [Dual Map]
   If $T \in \Lin{V, W}$, then the $\textbf{dual map}$ of $T$ is
   the linear map $T' \in \Lin{W', V'}$ defined by $T'(\phi) = \phi \circ T$
   for $\phi \in W'$.
\end{definition}

Let's make sure this makes sense with respect to the objects we're talking about.

$T$ maps from $V$ to $W$, and $\phi$ maps from $W$ to $\F$.
Thus, their composition maps from $V$ to $\F$, thus $\phi \circ T \in V'$.

\begin{example}
   Define $D : \Polys{\R}{\empty} \to \PolysAll{\R}$ by $Dp = p'$.
   
   Suppose $\phi$ is the linear functional on $\PolysAll{\R}$ defined
   by $\phi(p) = p(3)$.
   Then $D'(\phi)$ is the linear functional on $\PolysAll{R}$ given by
   \[ (D'(\phi))(p) = (\phi \circ D)(p) = \phi(Dp) = \phi(p') = p'(3) \]

   Thus $D'(\phi)$ is the linear functional on $\PolysAll{\R}$ that takes
   $p$ to $p'(3)$.
\end{example}

\begin{theorem} [Algebraic Properties of Dual Maps]
   We have the following familiar properties of linearity, with a new one added in:
   \begin{itemize}
      \item $(S + T)' = S' + T'$ for all $S, T \in \Lin{V, W}$
      \item $(\lambda T)' = \lambda T'$ for all $\lambda \in \F$ and all $T \in \Lin{V, W}$
      \item $(ST)' = T'S'$ for all $T \in \Lin{U, V}$ and all $S \in \Lin{V, W}$
   \end{itemize}
\end{theorem}

\begin{definition} [Annihilator]
   For $U \subseteq V$, the $\textbf{annihilator}$ of $U$, denoted
   $U^0$, is defined by
   \[ U^0 = \{ \phi \in V' : \phi(u) = 0 \text{for all } u \in U \} \]
\end{definition}

\begin{example} Examples of Annihilators 
   \begin{itemize}
      \item $\{0\}^0 = V'$ and $V^0 = \{0\}$
      \item Suppose $U = x^2 \PolysAll{\R} \subseteq \PolysAll{\R}$. Let
      $\phi$ be the linear functional on $\PolysAll{\R}$ defined by $\phi(p) = p'(0)$.
      Then $\phi \in U^0$.
   \end{itemize}
\end{example}

\begin{theorem}
   Suppose $U \subseteq V$. Then $U^0$ is a subspace of $V'$.
\end{theorem}

\begin{theorem}
   Suppose $V$ is finite-dimensional and $U$ is a subspace of $V$.
   Then $\dim U + \dim U^0 = \dim V$.
\end{theorem}

\begin{theorem}
   Suppose $V$ and $W$ are finite-dimensional and $T \in \Lin{V, W}$. Then
   \begin{itemize}
      \item $\Null T' = (\range T)^0$
      \item $\range T' = (\Null T)^0$
   \end{itemize}
\end{theorem}

\begin{theorem} [$T$ surjective is equivalent to $T'$ injective]
   Suppose $V$ and $W$ are finite-dimensional and $T \in \Lin{V, W}$. Then
   \begin{itemize}
      \item $T$ is surjective if and only if $T'$ is injective
      \item $T$ is injective if and only if $T'$ is surjective
   \end{itemize}
\end{theorem}

\begin{theorem}
   Suppose $V$ and $W$ are finite=dimensional and $T \in \Lin{V, W}$. Then
   \[ \dim \range T' = \dim \range T \]

   \begin{proof*}
      \begin{align*}
         \dim \range T' &= \dim W' - \dim \Null T' \\
         &= \dim W - \dim(\range T)^0 \\
         &= \dim \range T
      \end{align*}
   \end{proof*}
\end{theorem}

\begin{definition} [Transpose]
   The \textbf{transpose} of matrix $A$, denoted $A^t$, is the matrix
   obtained from $A$ by interchanging the rows and columns. More specificially,
   if $A$ is an $m$-by-$n$ matrix, then $A^t$ is the $n$-by-$m$ matrix whose entries are
   given by the equation:
   \[ (A^t)_{k,j} = A_{j,k} \]
\end{definition}

\begin{theorem}
   If $A$ is an $m$-by-$n$ matrix and $C$ is an $n$-by-$p$ matrix, then
   \[ (AC)^t = C^t A^t \]
\end{theorem}

\begin{theorem}
   Suppose $V$ and $W$ are finite-dimensional and $T \in \Lin{V, W}$. Then
   \[ \Matof{T'} = (\Matof{T})^t \]
   (provided we use a basis of $V$ and $W$ on the right and the exact
   dual bases of those bases on the left).
\end{theorem}

\begin{definition} [Row Rank, Column Rank]
   Suppose $A$ is an $m$-by-$n$ matrix with entries in $\F$.
   \begin{itemize}
      \item The \textbf{row rank} of $A$ is the dimension of the span of the rows of $A$ in $\F^{1, n}$
      \item The \textbf{column rank} of $A$ is the dimension of the span of the columns of $A$ in $\F^{m, 1}$.
   \end{itemize}
\end{definition}

\begin{theorem} [Dimension of $\range T$ equals column rank of $\Matof{T}$]
   Suppose $V$ and $W$ are finite-dimensional vector spaces and $T \in \Lin{V, W}$.
   Then $\dim \range T$ equals the column rank of $\Matof{T}$.
\end{theorem}

\begin{theorem}
   Suppose $A \in \F^{m, n}$. Then the row rank of $A$ equals the column rank
   of $A$.

   \begin{proof*}
      Define $T: \F^{n, 1} \to \F^{m, 1}$ by $Tx = Ax$. Thus $\Matof{T} = A$ (with
      respect to the standard bases). Now
      \begin{align*}
         \text{column rank of $A$} &= \text{column rank of $\Matof{T}$} \\
         &= \dim \range T \\
         &= \dim \range T' \\
         &= \text{column rank of $\Matof{T'}$} \\
         &= \text{column rank of $A^t$}
         &= \text{row rank of $A$}
      \end{align*}
   \end{proof*}
\end{theorem}

Thus, we can collapse everything into one term: rank.
\begin{definition} [Rank]
   The \textbf{rank} of $A$ is just the column rank of $A$.
\end{definition}
\endinput