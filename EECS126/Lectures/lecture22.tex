\subsection{Lecture 22}
\subsubsection{Proof of Neyman-Pearson Theorem}

We present a proof of the Neyman-Pearson Theorem, now that we've seen it in action. First, we note some intuition.
Observe that  $\lambda$ decreases, then we decide $\hat{X} = 1$ more frequently. This means that the PCD and PFA will go up.
Then what we do is we set $\lambda, \gamma$ such that PFA = $\beta$.

\begin{proof}[Neyman-Pearson]
    As a reminder, remember our objective.
    \begin{itemize}
        \item Maximize Probability of Correct Detection (PCD), $\Pr{\hat{X} = 1 \mid X = 1}$
        \item Subject to a constraint on Probability of False Alarm (PFA), $\Pr{\hat{X} = 1 \mid X = 0} \leq \beta$
    \end{itemize}

    Consider some other feasible decision rule $\tilde{X}$ with $\Pr{\tilde{X} = 1 \mid X = 0} \leq \beta$ and let the decision rule
    returned by N-P criterion be $\hat{X}$. We will show that the PCD of $\tilde{X}$ is
    $\leq$ the PCD of $\hat{X}$.

    Now the magical insight. Observe $(\hat{X} - \tilde{X})(L(Y) - \lambda) \geq 0$. To see this, consider three cases:
    \begin{enumerate}
    \item Suppose $L(y) = \lambda$; then the left side is 0 and this is trivially true.
    \item Suppose $L(y) > \lambda$; then $\hat{X} = 1 \geq \tilde{X}$ by the N-P decision rule, so both terms in the product are non-negative, meaning the left side is non-negative.
    \item Suppose $L(y) < \lambda$; then $\hat{X} = 0 \leq \tilde{X}$ by the N-P decision rule, so both terms in the product are non-positive, meaning the left side is non-negative.
    \end{enumerate}

    Now, we take the expectation on both sides given the null hypothesis (which we can do since expectation is monotone).
    \begin{align*}
        \E{(\hat{X} - \tilde{X})(L(Y) - \lambda) \mid X = 0} &\geq 0 \\
        \E{\hat{X} L(Y) \mid X = 0} - \E{\tilde{X} L(Y) \mid X = 0} &\geq \lambda(\E{\hat{X} \mid X = 0} - \E{\tilde{X} \mid X = 0}) \\
        \E{\hat{X} L(Y) \mid X = 0} - \E{\tilde{X} L(Y) \mid X = 0} &\geq \lambda(\Pr{\hat{X} = 1 \mid X = 0} - \Pr{\tilde{X} = 1 \mid X = 0}) \\
        \E{\hat{X} L(Y) \mid X = 0} - \E{\tilde{X} L(Y) \mid X = 0} &\geq 0 \\
        \E{\hat{X} L(Y) \mid X = 0} &\geq \E{\tilde{X} L(Y) \mid X = 0} \\
    \end{align*}
    The last step is because we assume $\Pr{\hat{X} = 1 \mid X = 0} = \beta$ (it is tight) and $\Pr{\tilde{X} = 1 \mid X = 0} \leq \beta$.

    Furthermore, we note that for any $g(Y)$:
    \begin{align*}
        \E{g(Y) L(Y) \mid X = 0} &= \int g(y) L(y) f_{Y \mid X}(y \mid 0) \dd{y} \\
        &= \int g(y) \frac{f_{Y \mid X}(y \mid 1)}{f_{Y \mid X}(y \mid 0)} f_{Y \mid X}(y \mid 0) \dd{y} \\
        &= \int g(y) f_{Y \mid X}(y \mid 1) \dd{y} \\
        &= \E{g(Y) \mid X = 1} \\
    \end{align*}
    The same holds for any $g(Y, Z)$ where $Z$ is independent of $X, Y$. (Think of the $Z$ as the randomization in $\hat{X}$)

    Thus, our inequality becomes:
    \begin{align*}
        \E{\hat{X} \mid X = 1} &\geq \E{\tilde{X} \mid X = 1} \\
        \Pr{\hat{X} = 1 \mid X = 1} &\geq \Pr{\tilde{X} = 1 \mid X = 1} \\
    \end{align*}

    Thus, $\hat{X}$ is optimal.
\end{proof}

\subsubsection{Jointly-Gaussian Random Variables}
We switch gears to jointly-Gaussian random variables. First, we define a certain notion for multivariate distributions.

\begin{definition}[Covariance Matrix]
    The covariance matrix $\Sigma_X$ of a random variable $X$ is defined as:
    \[ \Sigma_Y = \E{(Y - \E{Y})(Y - \E{Y})^T} \]
    Expressing this elementwise is:
    \begin{align*}
        \Sigma_Y(i, j) &= \Cov{Y_i, Y_j} \\
        \Sigma_Y(i, j) &= \E{(Y_i - \E{Y_i})(Y_j - \E{Y_j})} \\
        \Sigma_Y(i, j) &= \E{Y_i Y_j} - \E{Y_i} \E{Y_j} \\
        \Sigma_Y(i, i) &= \Var{Y_i}
    \end{align*}
\end{definition}

\begin{definition}[Jointly-Gaussian RVs]
    $Y_1, \dots, Y_n$ are jointly-Gaussian random variables if 
    \[Y = \begin{bmatrix}
        Y_1 \\ \vdots \\ Y_n
    \end{bmatrix} = AZ + \mu_Y\]
    where $Z_i$'s in $Z = \begin{bmatrix}
        Z_1 \\ \vdots \\ Z_k
    \end{bmatrix}$ are IID $\mathcal{N}(0, 1)$ and $A$ and $\mu_Y$ are $n$-by-$k$ and $n$-by-1, respectively.
\end{definition}

\begin{theorem}[PDF of Joint-Gaussians]
    Assuming $\Sigma_Y$ is nonsingular,
    \[ f_Y(y) = \frac{1}{\sqrt{(2 \pi)^n \det \Sigma_Y}} \exp\qty(-\frac{1}{2} (y - \mu_Y)^T \Sigma_Y^{-1} (y - \mu_Y)) \]
\end{theorem}

\begin{theorem}[Mean and Covariance of J-G]
    The mean of $Y$ is $\mu_Y$. The covariance of $Y$ is $\Sigma_Y = A A^T$. We write $\Normal{Y}{\mu_Y}{\Sigma_Y}$.
\end{theorem}

Let's prove these two facts.
\begin{proof*}
    Recall the change of variables formula. If two random variables $U, V$ are related as $V = aU + b$ with $a, b$ constant,
    then $f_V(v) = \frac{1}{|a|} f_U(u)$. We can extend this to the vector case. Suppose $A, B$ are constant matrices. Then:
    \[ V = AU + B \implies f_V(v) = \frac{1}{|\det A|} f_U(u) \]

    Consider the PDF of $Z$:
    \begin{align*}
        f_Z(z) &= \prod_{i = 1}^n \frac{1}{\sqrt{2\pi}} \exp\qty(-\frac{z_i^2}{2}) \\
        &= \frac{1}{\sqrt{(2\pi)^n}} \exp\qty(-\frac{||z||^2}{2}) \\
    \end{align*}
    Thus, since $Y = AZ + \mu_Y$, we have, $z = A^{-1} (y - \mu_Y)$ (assuming $A$ is invertible). This means that:
    \begin{align*}
        ||z||^2 &= ||A^{-1}(y - \mu_Y)||^2 \\
        &= (y - \mu_Y)^T \qty(A^{-1})^T A^{-1} (y - \mu_Y) \\
    \end{align*}

    In addition,
    \begin{align*}
        \Sigma_Y &= \E{(Y - \mu_Y) (Y - \mu_Y)^T} \\
        &= \E{(AZ) (AZ)^T} \\
        &= \E{A Z^T Z A^T} \\
        &= A \Sigma_Z A^T \\
        &= A A^T \\
    \end{align*}
    where we realize that $\Sigma_Z = I$ because each $Z_i$ is iid with variance 1. This means that:
    $\det \Sigma_Y = \det A \cdot \det A^T = (\det A)^2$. This shows that $\Sigma_Y$ is full rank if and only if $A$ is full rank.

    Also, 
    \begin{align*}
        (A^{-1})^T A^{-1} &= (A^T)^{-1} A^{-1} \\
        &= (AA^T)^{-1} \\
        &= \Sigma_Y^{-1}
    \end{align*}

    Thus, by change of variables formula:
    \begin{align*}
        f_Y(y) &= \frac{1}{\det A} \frac{1}{\sqrt{(2\pi)^n}} \exp\qty(-\frac{||z||^2}{2}) \\
        f_Y(y) &= \frac{1}{\sqrt{(2\pi)^n |\det \Sigma_Y|}} \exp\qty(-\frac{1}{2} (y - \mu_Y)^T \Sigma_Y^{-1} (y - \mu_Y))
    \end{align*}
    which is precisely the claim of the PDF.
\end{proof*}

\begin{note}
    Level curves for the Joint-Gaussian PDF are elliptical, with semi-axis lengths $\sigma_i$, center at $\mu_Y$, and some axis rotations (because the $\Sigma$ matrix is not diagonal).
\end{note}

\begin{example}[2-Dimensional J-Gs]
    Suppose $\mu_Y = 0$, $k = n = 2$. Then, we can write:
    \begin{align*}
        Y_1 &= a_{11} Z_1 + a_{12} Z_2 \\
        Y_2 &= a_{21} Z_1 + a_{22} Z_2
    \end{align*}
    i.e. the independent noises from $Z_1, Z_2$ is present in both of the $Y_i$'s, so they're not independent.
    However, it's clear from this expression that the $Y_i$'s are Gaussian (their marginal distribution) because it's a linear combination of two Gaussians.
\end{example}

\begin{example}
    Suppose the $Y_i$'s are iid $\mathcal{N}(0, \sigma_i^2)$. Then, we have:
    \[ \begin{bmatrix}
        Y_1 \\ Y_2
    \end{bmatrix} = \begin{bmatrix}
        \sigma_1 & 0 \\ 0 & \sigma_2
    \end{bmatrix} \begin{bmatrix}
        Z_1 \\ Z_2
    \end{bmatrix}\]

    This means the covariance matrix looks like:
    \[ \Sigma_Y = \begin{bmatrix}
        \sigma_1^2 & 0 \\ 0 & \sigma_2^2
    \end{bmatrix} \]
    Which means its inverse is:
    \[ \Sigma_Y^{-1} = \begin{bmatrix}
        \frac{1}{\sigma_1^2} & 0 \\ 0 & \frac{1}{\sigma_2^2}
    \end{bmatrix} \] 

    Then, using the form of the joint PDF, we can write:
    \begin{align*}
        f_Y(y_1, y_2) &= \frac{1}{2 \pi \sigma_1 \sigma_2} \exp\qty(-\frac12 \begin{bmatrix}
            y_1 & y_2
        \end{bmatrix} \begin{bmatrix}
            \frac{1}{\sigma_1^2} & 0 \\ 0 & \frac{1}{\sigma_2^2}
        \end{bmatrix} \begin{bmatrix}
            y_1 \\ y_2
        \end{bmatrix}) \\
        &=\frac{1}{2 \pi \sigma_1 \sigma_2} \exp\qty(-\frac12 \qty(\frac{y_1^2}{\sigma_1^2} + \frac{y_2^2}{\sigma_2^2})) \\
        &= \frac{1}{\sigma_1 \sqrt{2 \pi}}\exp\qty(-\frac{y_1^2}{2\sigma_1^2}) \cdot \frac{1}{\sigma_2 \sqrt{2 \pi}} \exp\qty(-\frac{y_2^2}{2\sigma_2^2})  \\
        &= f_{Y_1}(y_1) f_{Y_2}(y_2)
    \end{align*}
\end{example}

\begin{theorem}[Marginal Distribution of J-G RVs]
    If $Y_1, \dots, Y_n$ are J-G, then the marginal distribution of $Y_1, \dots, Y_n$ is Gaussian.
\end{theorem}

The converse of this does not always hold. You can construct an example where the random variables are marginally Gaussian, but not Jointly Gaussian.

\begin{example}
    Consider:
    \begin{align*}
        f_Y(y_1, y_2) &= \frac{1}{\pi \sigma_1 \sigma_2} \exp \qty(-\frac12 \qty(\frac{y_1^2}{\sigma_1^2} + \frac{y_2^2}{\sigma_2^2})) \\
    \end{align*}
    where the density is only defined where $y_1 \cdot y_2 > 0$ (first and third quadrants). Note that this is not Jointly-Gaussian because the level 
    curves cannot be elliptical--namely there is no component in the second and fourth quadrants. But, we have the following marginal for $y_1 > 0$:
    \begin{align*}
        f_{Y_1}(y_1) &= \frac{1}{\pi \sigma_1 \sigma_2} \int_0^{\infty} \exp\qty(-\frac12 \qty(\frac{y_1^2}{\sigma_1^2} + \frac{y_2^2}{\sigma_2^2})) \dd{y_2} \\
        &= \frac{1}{\pi \sigma_1 \sigma_2} \cdot \frac{1}{2} \sqrt{2\pi}  \sigma_2 \cdot \exp\qty(-\frac12 \frac{y_1^2}{\sigma_1^2}) \\
        &= \frac{1}{\sqrt{2 \pi} \sigma_1} \exp \qty(-\frac{y_1^2}{2 \sigma_1^2}) \\
    \end{align*}
    You also have the same marginal for $y_1 \leq 0$, so $Y_1$ is marginally Gaussian (and so is $Y_2$, by symmetry). Thus we have found this tricky example.
\end{example}

We also have the following result.

\begin{theorem}
    If $Y_1, \dots, Y_n$ are Jointly-Gaussian RVs, then they are mutually independent if and only if they are pairwise uncorrelated.    
\end{theorem}

\begin{example}
    Suppose $X$ and $Y$ and are independent $\mathcal{N}(0,1)$. Consider the random variables $X + Y$ and $X- Y$.
    \begin{align*}
        \begin{bmatrix}
            X + Y \\ X - Y
        \end{bmatrix} &= \begin{bmatrix}
            1 & 1 \\ 1 & - 1
        \end{bmatrix} \begin{bmatrix}
            X \\ Y
        \end{bmatrix}
    \end{align*}
    This means $X + Y$ and $X - Y$ are jointly-Gaussian.
\end{example}

\begin{theorem}
    If $V$ and $W$ are jointly-Gaussian, then their linear combinations $AV + a$ and $BW + b$ are also jointly-Gaussian for some constant matrices $A, B$ and
    some constant vectors $a$ and $b$.
\end{theorem}