\subsection{Lecture 12}

\subsubsection{Limits of Distributions}
Now we discuss results which are applications of Characteristic Functions.

\begin{theorem}
    Consider fixed $\lambda > 0$. Then we have,
    \[ \lim_{n \to \infty} B\qty(n, \lambda/n) = P(\lambda) \]

    \begin{proof*}
        Although this is easy to see by taking the limit of PDFs, we opt
        for characteristic functions instead.
        
        Let
        \[ X_n \equiv_D B(n, \lambda/n) \]
        Then,
        \begin{align*}
            \phi_{X_n}(u) &= \E{e^{iuX_n}} \\
            &= \E{e^{iu(Z_n(1) + \dots + Z_n(n))}}
        \end{align*}
        where $Z_n(i)$ are iid $B\qty(\lambda/n)$.
        \begin{align*}
            \phi_{X_n}(u) &= \prod_{i = 1}^n \E{e^{iuZ_n(i)}}\\
            &= \qty(\frac{\lambda}{n} e^{iu0} + \qty(1 - \frac{\lambda}{n}) e^{iu1})^n \\
            &= \qty(1 + \frac{\lambda}{n}\qty(e^{iu} - 1))^n \\
            &\to_{n \to \infty} \exp(\lambda(e^{iu} - 1))
        \end{align*}
        However, if $X \equiv_D P(\lambda)$, then its characteristic function is:
        \begin{align*}
            \phi_X(u) &= \sum_{m = 0}^{\infty} \frac{\lambda^m e^{-\lambda}}{m!} e^{ium} \\
            \phi_X(u) &= e^{-\lambda} \sum_{m = 0}^{\infty} \frac{\qty(\lambda e^{iu})^m}{m!} \\
            \phi_X(u) &= e^{-\lambda} \exp(\lambda e^{iu}) \\
            \phi_X(u) &= \exp(\lambda \qty(e^{iu} - 1))
        \end{align*}

        So the characteristic function $\phi_{X_n}(u)$ converges to $\phi_{X}(u)$, so there is a convergence in distribution.
    \end{proof*}
\end{theorem}

The next result is similar, and can be proved in the same way (proof omitted here).

\begin{theorem}
    Consider fixed $\lambda > 0$. Then we have,
    \[ \lim_{n \to \infty} \frac{G\qty(\frac{\lambda}{n})}{n} = Exp(\lambda) \]
\end{theorem}

Here are some more results of manipulation:
\begin{theorem}
    Let $\Normal{X}{0}{1}$ and $\Normal{Y}{0}{1}$ be independent. Then: $\Expo{X^2 + Y^2}{1/2}$.

    \begin{proof*}
        Let $Z = X^2 + Y^2$. Then:
        \begin{align*}
            \Pr{Z \leq z} &= \Pr{X^2 + Y^2 \leq z} \\
            &= \frac{1}{2\pi} \int \int_{X^2 + Y^2 \leq z} e^{-(x^2 + y^2)/2} \dd{x} \dd{y} \\
            &= \frac{1}{2\pi} \int_{0}^{2 \pi} \int_{0}^{\sqrt{z}} e^{-r^2/2} r \dd{r} \dd{\theta} \\
            &= \int_{0}^{\sqrt{z}} r e^{-r^2/2} \dd{r} \\
            &= - e^{-r^2/2} \Big|_0^{\sqrt{z}} \\
            &= 1 - e^{-z/2}
        \end{align*}
        Which exactly the CDF of an exponential.
    \end{proof*}
\end{theorem}

\begin{theorem}
    Consider $\Normal{X}{0}{1}$. Let $Q(x) = P(X > x)$ (called the error function). Then:
    \[\frac{x}{1 + x^2} f_X(x) \leq Q(x) \leq \frac{1}{x} f_X(x)\]
\end{theorem}

Now we return to the application of multiplexing. If there is a lot of traffic at an output port of a switch,
this can cause buffering (building a buffer of things to transmit next, if there's too many to transfer all at once).
How can we model buffering? Let's try using a discrete-time Markov chain.

Suppose that at each instant $n^+$, a packet arrives with probability $\lambda \in [0, 1]$ independently
of the past. The time to transmit a packet is geometrically distributed with parameter $\mu \in (0, 1]$. All transmission
times are independent. A packet in service completes transmission at $n^-$ with probability $\mu$.

However, for the purposes of this course, this DTMC model is too complicated. We instead use CTMCs (Continuous-Time
Markov Chains).

\section{Networks}

\subsection{Lecture 12, Continued}

We start our discussion of networks with the familiar Markov Chains, but with a twist.

\begin{definition}[Infinite DTMC]
    You can consider $\{X(n), n \geq 0\}$ as a Markov Chain over a (countably) infinite state space $\mathcal{X} = \{0, 1, 2, \dots\}$.
    
    The initial distribution is $\pi(i), i \in \mathcal{X}$ such taht $\pi(i) \geq 0$ and $\sigma_i \pi(i) = 1$.
    The state transition probability matrix $P$ also exists, where $\sum_j P(i, j) = 1$.
\end{definition}

Irreducibility and aperiodicity are defined the same way as in the finite case. The balance equations also hold for
the invariant distribution, as $\pi = \pi P$.

\begin{definition}[Types of States]
    A state is \textbf{transient} if starting the Markov Chain from this state causes it to be visited finitely often. A state is 
    \textbf{recurrent} if it's not transient.

    A recurrent state is \textbf{positive recurrent} if the average time between successive visits is finite, otherwise
    it's \textbf{null recurrent}.
\end{definition}

Transience and recurrence make sense in finite DTMCs, but positive and null recurrence only make sense in infinite DTMCs.

\begin{theorem}
    For an irreducible DTMC, states are either all transient, all positive recurrent, or all null recurrent.
\end{theorem}

There is also the idea of communicating classes. Basically, these are
strongly connected components in the digraph representing the Markov chain. A communicating class is a maximal set of
states such that there is a nonzero probability path from any one state to any other.

A good example of an infinite DTMC is the random walk reflected at 0, i.e. at any natural number you can walk right or left with probabilities
$p$ and $1-p$ respectively; at 0 you walk
to the right with probability $p$ and stay in 0 with probability $1 - p$.

Then, we have the following:
\begin{itemize}
    \item If $p > 1/2$, then it is a transient MC.
    \item If $p = 1/2$, then it is a null-recurrent.
    \item If $p < 1/2$, then it is a positive-recurrent.
\end{itemize}
Let's prove these, setting $p + q = 1$.
\begin{proof*}
    \begin{itemize}
        \item $p > q$
        Let $Z(n)$ be iid with $\Pr{Z(n) = 1} = p$, $\Pr{Z(n) = -1} = q$. Now define:
        \begin{align*} 
            X(n) &= \max(X(n - 1) + Z(n), 0) \\
            X(n) &\geq X(0) + Z(1) + \dots + Z(n) \\
            \frac{X(n)}{n} &\geq \frac{X(0) + Z(1) + \dots + Z(n)}{n} \\
            &\to \E{Z}
        \end{align*}
        However, note that $\E{Z} = 1 \cdot p - \cdot q > 0 $, so $X(n) \to \infty$, meaning
        the Markov chain is transient.
    \end{itemize}

\end{proof*}


We also have to modify the big theorem for infinite DTMCs.

\begin{theorem}
    Consider an irreducible DTMC $\{X(n), n \geq 0\}$ over an infinite state space with an invariant
    distribution $\pi$. Then for each $i, \pi(i) = \frac{1}{\E{T_i \mid X(0) = i}}$ where $T_i$ is
    the first time after 0 to reach state $i$. Furthermore,

    \begin{enumerate}
        \item If the markov chain is positive recurrent, there is a unique invariant distribution $\pi$ where
        \begin{align*}
            \lim_{N \to \infty} \frac{1}{N} \sum_{n = 0}^{N - 1} \mathbbm{1}_{\{X(n) = i\}} = \pi(i)
        \end{align*}
        i.e. the long-term fraction of time converges to the invariant distribution.
        \item If the markov chain is positive recurrent and aperiodic, $\pi_n \to \pi$ regardless of the initial distribution.
        \item If the markov chain is not positive recurrent, it does not have an invariant distribution, and the
        fraction of time spent in any one state goes to 0.
    \end{enumerate}
\end{theorem}

We again try to apply to balance equations to our random walk reflected at 0. However,
we can consider many states at once as a kind of "meta-state." Then, we track only the "net" flow in and out
of our "meta-state." These must be equal in the balance equations (think of the internal transitions as internal forces in mechanics;
by Newton's second law, these cancel out).

Let us make our meta-state $\{0, 1, \dots, n\}$. The flow out is $p \pi(n)$ and the flow in is $(1- p) \pi(n + 1)$. Calling
$\rho = \frac{p}{1-p}$. This means:
\begin{align*}
    p \pi(n) &= (1 - p) \pi(n + 1) \\
    \pi(n) &= \rho^n \pi(0) \\
    \sum_{i = 0}^{\infty} \rho^i \pi(0) &= 1 \\
    \pi(0) &= \frac{1}{1 - \rho}
\end{align*}
Note that if $p = 1/2$, then $\rho = 1$.

Now, we define another condition:

\begin{definition}[Detailed Balance Equations]
    The condition $\pi(i) P(i, j) = \pi(j) P(j, i)$ for any two
    states $i$ and $j$ are called detailed balance equations.
\end{definition}

This notion will be important later (when we discuss reversibility).
